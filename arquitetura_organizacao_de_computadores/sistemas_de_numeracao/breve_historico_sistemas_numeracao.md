## 1. Breve Histórico da Representação da Informação

A humanidade sempre buscou formas de registrar dados, evoluindo de símbolos físicos para sinais eletrônicos:

* **Antiguidade:** O uso do ábaco e sistemas de contagem em base 10 (decimal) devido aos dez dedos das mãos.
* **Lógica Booleana (1854):** George Boole criou um sistema lógico onde tudo se resume a **Verdadeiro ou Falso**. Esse é o alicerce da programação moderna e da cibersegurança.


* **Claude Shannon (1937):** Ele provou que a lógica booleana e a aritmética poderiam ser aplicadas a circuitos elétricos (relés), definindo o **Bit** (Binary Digit) como a menor unidade de informação.

## 2. Sistemas de Numeração

Os computadores utilizam sistemas diferentes do nosso cotidiano para otimizar o hardware:

* **Binário (Base 2):** Utiliza apenas **0 e 1**. É a linguagem nativa do processador (presença ou ausência de tensão elétrica).
* **Hexadecimal (Base 16):** Utiliza números de 0 a 9 e letras de A a F. É amplamente usado no **Back-End** e em redes para representar cores, endereços de memória e chaves criptográficas de forma mais curta que o binário.
* **Octal (Base 8):** Menos comum hoje, mas importante na história para representar permissões de arquivos em sistemas Unix.

## 3. Aritmética Computacional

Diferente da nossa matemática no papel, a aritmética dentro do chip precisa lidar com limitações físicas:

* **Adição e Subtração Binária:** Seguem regras simples (como , onde o "1" sobe para a próxima casa), mas são a base para qualquer cálculo de algoritmo.
* **Complemento de Dois:** A técnica genial que os computadores usam para representar **números negativos** e realizar subtrações usando apenas somadores.
* **Ponto Flutuante (IEEE 754):** Como o computador representa números reais (com vírgula). Erros nessa representação podem causar falhas críticas em sistemas de alta precisão.

---
